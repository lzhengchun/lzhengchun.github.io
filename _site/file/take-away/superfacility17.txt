Modern scientific experimental facilities such as x-ray light sources increasingly require on-demand access to large-scale computing for data analysis, for example to detect experimental errors or to select the next experiment.
As the number of such facilities, the number of instruments at each facility, and the scale of computational demands all grow, the question arises as to how to meet these demands most efficiently and cost-effectively.

A single computer per instrument is unlikely to be cost-effective because of low utilization and high operating costs.
A single national compute facility, on the other hand, introduces a single point of failure and perhaps excessive communication costs.
We introduce here methods for evaluating these and other potential design points, such as per-facility computer systems and a distributed multisite ``superfacility." 

We use the U.S. Department of Energy light sources as a use case and build a mixed-integer programming model and a customizable superfacility simulator to enable joint optimization of design choices and associated operational decisions.  
The methodology and tools provide new insights into design choices for on-demand computing facilities for real-time analysis of scientific experiment data. 

The customizable simulator developed in this work is also useful in superfacility operations, for example (1) to work as a part to decision support system to evaluate proposals for unforeseen scenarios; (2) to prove the feasibility of new project in terms of its computing requirement; and (3) to study scheduling algorithms for workflows run over distributed infrastructure. 